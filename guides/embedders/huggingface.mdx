---
title: Semantic Search with Hugging Face Inference Endpoints - Meilisearch documentation
description: This guide will walk you through the process of setting up Meilisearch with Hugging Face Inference Endpoints.
---

# Semantic search with Hugging Face Inference Endpoints

## Introduction

This guide will walk you through the process of setting up Meilisearch with [Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) embeddings to enable semantic search capabilities.

## Requirements

To follow this guide, you'll need:

- A [Meilisearch Cloud](https://www.meilisearch.com/cloud) project running version 1.10 or above with the Vector store activated
- A [Hugging Face account](https://huggingface.co/) with a deployed inference endpoint
- The endpoint URL and API key of the deployed model on your Hugging Face account

## Configure the embedder

Set up an embedder using the update settings endpoint:

```json
{
  "hf-inference": {
    "source": "rest",
    "url": "ENDPOINT_URL",
    "apiKey": "API_KEY",
    "dimensions": 384,
    "documentTemplate": "CUSTOM_LIQUID_TEMPLATE",
    "request": {
      "inputs": ["{{text}}", "{{..}}"]
    },
    "response": ["{{embedding}}", "{{..}}"]
  }
}
```

In this configuration:

- `source`: Specifies the source of the embedder, which is set to "rest" for using a REST API.
- `url`: Replace `<Endpoint URL>` with your actual deployed model endpoint on Hugging Face.
- `apiKey`: Replace `<API Key>` with your actual Hugging Face API key.
- `dimensions`: Specifies the dimensions of the embeddings. Set to 384 for `baai/bge-small-en-v1.5` in our example.
- `documentTemplate`: Optionally, you can provide a [custom template](/learn/ai_powered_search/getting_started_with_ai_search?utm_campaign=vector-search&utm_source=docs&utm_medium=huggingface-embeddings-guide#documenttemplate) for generating embeddings from your documents.
- `request`: Defines the request structure for the deployed model API, including the input parameters.
- `response`: Defines the expected response structure from the deployed model API, including the embedding data.

Once you've configured the embedder settings, Meilisearch will automatically generate embeddings for your documents and store them in the vector store.

It's recommended to monitor the tasks queue to ensure everything is running smoothly. You can access the tasks queue using the Cloud UI or the [Meilisearch API](/reference/api/tasks?utm_campaign=vector-search&utm_source=docs&utm_medium=huggingface-embeddings-guide#get-tasks).

## Perform a semantic search

With the embedder set up, you can now perform semantic searches. Make a search request with the `hybrid` search parameter, setting `semanticRatio` to `1`:

```json
{
  "q": "QUERY_TERMS",
  "hybrid": {
    "semanticRatio": 1,
    "embedder": "hf-inference"
  }
}
```

In this request:

- `q`: the search query
- `hybrid`: enables AI-powered search functionality
  - `semanticRatio`: controls the balance between semantic search and full-text search. Setting it to `1` means you will only receive semantic search results
  - `embedder`: the name of the embedder used for generating embeddings

## Conclusion

By following this guide, you should now have Meilisearch set up with Hugging Face Inference Endpoints, enabling you to leverage semantic search capabilities in your application. Meilisearch's auto-batching and efficient handling of embeddings make it a powerful choice for integrating semantic search into your project.

Consult the [embedder setting documentation](/reference/api/settings#embedders-experimental) for more information on other embedder configuration options.
